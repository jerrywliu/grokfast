{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f92841d-bd39-4472-8faf-b37a0e57d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from argparse import ArgumentParser\n",
    "from itertools import permutations\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pyhessian import hessian\n",
    "\n",
    "from grokfast import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2d182-8614-4bcf-998b-16498ca499a1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8126ca6e-ba03-4c8f-b0cc-65efb9cc1f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Causal transformer block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_mask = torch.full(\n",
    "            (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n",
    "        )\n",
    "        attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "        attn_mask[torch.isnan(attn_mask)] = 0.0 # fixes all 'nan' on 'mps' device\n",
    "\n",
    "        x = self.ln_1(x)\n",
    "        a, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        x = x + a\n",
    "        m = self.mlp(self.ln_2(x))\n",
    "        x = x + m\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Causal Transformer decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=128, num_layers=2, num_heads=4, num_tokens=97, seq_len=5):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_tokens, dim)\n",
    "        self.position_embeddings = nn.Embedding(seq_len, dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(Block(dim, num_heads))\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, num_tokens, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.token_embeddings(x)\n",
    "        positions = torch.arange(x.shape[0], device=x.device).unsqueeze(-1)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        h = self.ln_f(h)\n",
    "        logits = self.head(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa92a50-e8a1-4f4e-9ee6-0a6f00522abe",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d430c283-efc4-435e-a853-80e766caf255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mod_p_data(p, eq_token, op_token, task=\"multiplication\"):\n",
    "    \"\"\"x◦y = x/y (mod p) for 0 ≤ x < p, 0 < y < p\n",
    "    \"\"\"\n",
    "    x = torch.arange(p)\n",
    "    y = torch.arange(1, p)\n",
    "    x, y = torch.cartesian_prod(x, y).T\n",
    "\n",
    "    eq = torch.ones_like(x) * eq_token\n",
    "    op = torch.ones_like(x) * op_token\n",
    "    \n",
    "    if task == \"multiplication\":\n",
    "        result = (x * y) % p\n",
    "    elif task == \"addition\":\n",
    "        result = (x + y) % p\n",
    "    elif task == \"subtraction\":\n",
    "        result = (x - y) % p\n",
    "    elif task == \"division\":\n",
    "        y_inv = pow(y, p-2, p)\n",
    "        return (x * y_inv) % p\n",
    "    elif task == \"parity_division\":\n",
    "        if (y % 2) != 0:\n",
    "            # Division\n",
    "            y_inv = pow(y, p-2, p)\n",
    "            return (x * y_inv) % p\n",
    "        else:\n",
    "            # Subtraction\n",
    "            return (x - y) % p\n",
    "    elif task == \"sum_of_squares\":\n",
    "        result = (x**2 + y**2) % p\n",
    "    elif task == \"quad1\":\n",
    "        result = (x**2 + x*y + y**2) % p\n",
    "    elif task == \"quad2\":\n",
    "        result = (x**2 + x*y + y**2 + x) % p\n",
    "    elif task == \"cubic1\":\n",
    "        result = (x**3 + xy) % p\n",
    "    elif task == \"cubic2\":\n",
    "        result = (x**3 + x*(y**2) + y) % p\n",
    "\n",
    "    # \"All of our experiments used a small transformer trained on datasets of\n",
    "    # equations of the form a◦b = c, where each of “a”, “◦”, “b”, “=”, and “c”\n",
    "    # is a seperate token\"\n",
    "    return torch.stack([x, op, y, eq, result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27043fa-e3f3-4cc0-8f1a-1369d05f2673",
   "metadata": {},
   "source": [
    "## TODO: S5 experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761d9ab-36b4-42d0-b6d4-614af9703efc",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a09d1717-765c-46be-8838-54e04922e565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # tokens for <op> and <=>. It's not clear why <=> is needed at all since it\n",
    "    # has no effect on the output, but we'll leave it in to best follow the\n",
    "    # paper.\n",
    "    eq_token = args.p\n",
    "    op_token = args.p + 1\n",
    "\n",
    "    # \"We trained a standard decoder-only transformer (Vaswani et al., 2017)\n",
    "    # with causal attention masking, and calculated loss and accuracy only on\n",
    "    # the answer part of the equation. For all experiments we used a\n",
    "    # transformer with 2 layers, width 128, and 4 attention heads\"\n",
    "    model = Decoder(\n",
    "        dim=128, num_layers=2, num_heads=4, num_tokens=args.p + 2, seq_len=5\n",
    "    ).to(device)\n",
    "    nparams = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "    print(model)\n",
    "    print(f'Total number of parameters: {nparams}')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    data = multiplication_mod_p_data(args.p, eq_token, op_token)\n",
    "\n",
    "    train_idx, valid_idx = torch.randperm(data.shape[1]).split(data.shape[1] // 2)\n",
    "    train_data, valid_data = data[:, train_idx], data[:, valid_idx]\n",
    "    print(f\"Train data: {train_data.shape}\")\n",
    "    print(f\"Valid data: {valid_data.shape}\")\n",
    "\n",
    "    # For most experiments we used AdamW optimizer with learning rate 10−3,\n",
    "    # weight decay 1, β1 = 0.9, β2 = 0.98\n",
    "    optimizer = getattr(torch.optim, args.optimizer)(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "        betas=(args.beta1, args.beta2),\n",
    "    )\n",
    "\n",
    "    #  linear learning rate warmup over the first 10 updates\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda update: 1 if update > 10 else update / 10\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = math.ceil(train_data.shape[1] / args.batch_size)\n",
    "\n",
    "    its, train_acc, val_acc, train_loss, val_loss = [], [], [], [], []\n",
    "    hessian_its, train_hessiantrace, val_hessiantrace = [], [], []\n",
    "    grads = None\n",
    "    i = 0\n",
    "    \n",
    "    # Compute/save hessian condition\n",
    "    def save_hessian(e):\n",
    "        return (e < 20 or (e+1) % args.hessian_save_every == 0)\n",
    "    hessian_loss_func = lambda output, target: F.cross_entropy(output[-1], target)\n",
    "\n",
    "    # For logging network weights.\n",
    "    net_its, nets = [], []\n",
    "\n",
    "    print(f\"Epochs: {int(args.budget) // steps_per_epoch}\")\n",
    "    pbar = tqdm(range(int(args.budget) // steps_per_epoch))\n",
    "    for e in pbar:\n",
    "        \n",
    "        if save_hessian(e):\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Compute train Hessian\n",
    "                hessian_comp = hessian(model, hessian_loss_func, data=(train_data[:-1], train_data[-1]), cuda=(torch.cuda.is_available()))\n",
    "                train_trace = np.mean(hessian_comp.trace())\n",
    "                # print(f\"Train Hessian trace: {trace}\")\n",
    "                train_hessiantrace.append(train_trace)\n",
    "                hessian_its.append(i)\n",
    "                \n",
    "                # Compute valid Hessian\n",
    "                hessian_comp = hessian(model, hessian_loss_func, data=(valid_data[:-1], valid_data[-1]), cuda=(torch.cuda.is_available()))\n",
    "                val_trace = np.mean(hessian_comp.trace())\n",
    "                # print(f\"Valid Hessian trace: {trace}\")\n",
    "                val_hessiantrace.append(val_trace)\n",
    "                \n",
    "            pbar.set_description(f\"Train Hessian: {train_trace}, valid Hessian: {val_trace}\")\n",
    "        # randomly shuffle train data\n",
    "        train_data = train_data[:, torch.randperm(train_data.shape[1])]\n",
    "\n",
    "        for data, is_train in [(train_data, True), (valid_data, False)]:\n",
    "\n",
    "            model.train(is_train)\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            # torch.split faster than dataloader with tensor\n",
    "            dl = torch.split(data, args.batch_size, dim=1)\n",
    "            for input in dl:\n",
    "                input = input.to(device)\n",
    "                # print(f\"Input shape: {input.shape}\")\n",
    "\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits = model(input[:-1])\n",
    "                    # calculate loss only on the answer part of the equation (last element\n",
    "                    loss = F.cross_entropy(logits[-1], input[-1])\n",
    "                    total_loss += loss.item() * input.shape[-1]\n",
    "\n",
    "                if is_train:\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    #######\n",
    "\n",
    "                    trigger = i < 500 if args.two_stage else False\n",
    "\n",
    "                    if args.filter == \"none\":\n",
    "                        pass\n",
    "                    elif args.filter == \"ma\":\n",
    "                        grads = gradfilter_ma(model, grads=grads, window_size=args.window_size, lamb=args.lamb, trigger=trigger)\n",
    "                    elif args.filter == \"ema\":\n",
    "                        grads = gradfilter_ema(model, grads=grads, alpha=args.alpha, lamb=args.lamb)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Invalid gradient filter type `{args.filter}`\")\n",
    "\n",
    "                    #######\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    i += 1\n",
    "\n",
    "                acc = (logits[-1].argmax(-1) == input[-1]).float().mean()\n",
    "                total_acc += acc.item() * input.shape[-1]\n",
    "\n",
    "            if is_train:\n",
    "                train_acc.append(total_acc / train_data.shape[-1])\n",
    "                train_loss.append(total_loss / train_data.shape[-1])\n",
    "                its.append(i)\n",
    "            else:\n",
    "                val_acc.append(total_acc / valid_data.shape[-1])\n",
    "                val_loss.append(total_loss / valid_data.shape[-1])\n",
    "\n",
    "        if args.save_weights:\n",
    "            # do_save = e <= 500 or (e > 500 and (e + 1) % 100 == 0) or e == int(args.budget) // steps_per_epoch - 1\n",
    "            do_save = ((e + 1) % 500 == 0) or (e == int(args.budget) // steps_per_epoch - 1)\n",
    "        else:\n",
    "            do_save = (e + 1) % 100 == 0\n",
    "        if do_save:\n",
    "            \n",
    "            # Accuracy\n",
    "            steps = torch.arange(len(train_acc)).numpy() * steps_per_epoch\n",
    "            plt.plot(steps, train_acc, label=\"train\")\n",
    "            plt.plot(steps, val_acc, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Multiplication (training on 50% of data)\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.grid()\n",
    "            plt.savefig(f\"results/acc_{args.label}.png\", dpi=150)\n",
    "            plt.close()\n",
    "            \n",
    "            # Loss\n",
    "            plt.plot(steps, train_loss, label=\"train\")\n",
    "            plt.plot(steps, val_loss, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Multiplication (training on 50% of data)\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.grid()\n",
    "            plt.savefig(f\"results/loss_{args.label}.png\", dpi=150)\n",
    "            plt.close()\n",
    "            \n",
    "            # Hessian\n",
    "            plt.plot(hessian_its, [abs(trace) for trace in train_hessiantrace], label=\"train\")\n",
    "            plt.plot(hessian_its, [abs(trace) for trace in val_hessiantrace], label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Multiplication (training on 50% of data)\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Hessian trace\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.yscale(\"log\", base=10)\n",
    "            plt.grid()\n",
    "            plt.savefig(f\"results/hessiantrace_{args.label}.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "            results = {\n",
    "                'its': its,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'hessian_its': hessian_its,\n",
    "                'train_hessiantrace': train_hessiantrace,\n",
    "                'val_hessiantrace': val_hessiantrace,\n",
    "            }\n",
    "\n",
    "            if args.save_weights:\n",
    "                net_its.append(e)\n",
    "                nets.append(copy.deepcopy(model.state_dict()))\n",
    "                results['net_its'] = net_its\n",
    "                results['net'] = nets\n",
    "\n",
    "            torch.save(results, f\"results/res_{args.label}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f5578-16b0-4932-9520-63d1e6faa292",
   "metadata": {},
   "source": [
    "## Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b51a79dd-c9ab-4aff-9e95-192b321a0a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExptSettings:\n",
    "    def __init__(self):\n",
    "        self.label = \"base\"\n",
    "        self.seed = 0\n",
    "        # Data\n",
    "        self.p = 97\n",
    "        self.task = \"multiplication\"\n",
    "        \n",
    "        self.budget = 3e5\n",
    "        self.batch_size = 512\n",
    "        self.lr = 1e-3\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.98\n",
    "        self.weight_decay = 0\n",
    "        self.optimizer = \"Adam\"\n",
    "        # Grokfast\n",
    "        self.filter = \"none\"  # choices: [\"none\", \"ma\", \"ema\", \"fir\"]\n",
    "        self.alpha = 0.99\n",
    "        self.window_size = 100\n",
    "        self.lamb = 5.0\n",
    "        # Ablation studies\n",
    "        self.two_stage = False\n",
    "        self.save_weights = False\n",
    "        # Hessian\n",
    "        self.hessian_save_every = 20\n",
    "\n",
    "# Instantiate and set values\n",
    "args = ExptSettings()\n",
    "args.label = args.task = \"multiplication\"\n",
    "args.save_weights = True\n",
    "args.hessian_save_every = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d79b00-ad75-4797-a7d8-3cb0fd90aa72",
   "metadata": {},
   "source": [
    "# Run expt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "558a8b97-d81b-4689-b7cb-01050255dfe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment results saved under name: multiplication_none\n",
      "Decoder(\n",
      "  (token_embeddings): Embedding(99, 128)\n",
      "  (position_embeddings): Embedding(5, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x Block(\n",
      "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=128, out_features=99, bias=False)\n",
      ")\n",
      "Total number of parameters: 422784\n",
      "Device: cuda\n",
      "Train data: torch.Size([5, 4656])\n",
      "Valid data: torch.Size([5, 4656])\n",
      "Epochs: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Hessian: 12.425347566604614, valid Hessian: 7011.37275390625:   2%|▏         | 519/30000 [07:47<7:22:56,  1.11it/s]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     optim_suffix \u001b[38;5;241m=\u001b[39m optim_suffix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lrx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(args\u001b[38;5;241m.\u001b[39mlr\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExperiment results saved under name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mfilter_str\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mfilter_suffix\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39moptim_suffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 74\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     71\u001b[0m hessian_its\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Compute valid Hessian\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m hessian_comp \u001b[38;5;241m=\u001b[39m \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessian_loss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m val_trace \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(hessian_comp\u001b[38;5;241m.\u001b[39mtrace())\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# print(f\"Valid Hessian trace: {trace}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/icon/lib/python3.9/site-packages/pyhessian/hessian.py:74\u001b[0m, in \u001b[0;36mhessian.__init__\u001b[0;34m(self, model, criterion, data, dataloader, cuda)\u001b[0m\n\u001b[1;32m     72\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs)\n\u001b[1;32m     73\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets)\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# this step is used to extract the parameters from the model\u001b[39;00m\n\u001b[1;32m     77\u001b[0m params, gradsH \u001b[38;5;241m=\u001b[39m get_params_grad(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[0;32m~/.conda/envs/icon/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/icon/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filter_str = ('_' if args.label != '' else '') + args.filter\n",
    "window_size_str = f'_w{args.window_size}'\n",
    "alpha_str = f'_a{args.alpha:.3f}'.replace('.', '')\n",
    "lamb_str = f'_l{int(args.lamb)}'\n",
    "\n",
    "if args.filter == 'none':\n",
    "    filter_suffix = ''\n",
    "elif args.filter == 'ma':\n",
    "    filter_suffix = window_size_str + lamb_str\n",
    "elif args.filter == 'ema':\n",
    "    filter_suffix = alpha_str + lamb_str\n",
    "else:\n",
    "    raise ValueError(f\"Unrecognized filter type {args.filter}\")\n",
    "\n",
    "optim_suffix = ''\n",
    "if args.weight_decay != 0:\n",
    "    optim_suffix = optim_suffix + f'_wd{args.weight_decay:.1e}'.replace('.', '')\n",
    "if args.lr != 1e-3:\n",
    "    optim_suffix = optim_suffix + f'_lrx{int(args.lr / 1e-3)}'\n",
    "\n",
    "print(f'Experiment results saved under name: {args.label + filter_str + filter_suffix + optim_suffix}')\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be22c5-83a8-4f58-866c-328869c48a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icon",
   "language": "python",
   "name": "icon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
